{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Introduction to PyTorch and Behavior Cloning\n",
    "\n",
    "In this exercise, we introduce the fundamental concepts of PyTorch and show how this framework can be applied to build and train custom neural networks. \n",
    "We then utilize these networks to train an agent to play a simple video game from demonstrations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Install packages and dependencies\n",
    "\n",
    "We recommend using a ``conda`` environment (or virtual environments) for this and the follwing exercises. If you are not familiar with Anaconda, you can read this tutorial: [Getting started with conda]( https://conda.io/projects/conda/en/latest/user-guide/getting-started.html). \n",
    "\n",
    "Here's an example of the steps you can take:\n",
    "1. Install a conda distribution on your machine\n",
    "2. Create a new conda Python 3.9 environment with name rl_lab: ``conda create --name rl_lab python=3.9``\n",
    "3. Activate the environment: ``conda activate rl_lab``\n",
    "4. Install dependencies using the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install gymnasium==0.29.1\n",
    "!pip install minatar==1.0.15\n",
    "!pip install matplotlib\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction to PyTorch\n",
    "\n",
    "PyTorch is a Python-based library designed for deep learning. It is distinguished by its dynamic computational graph, which enables researchers and developers to construct models with a high degree of flexibility. PyTorch has found extensive use in various scientific and engineering domains due to its ease of use and extensive research-friendly features.\n",
    "\n",
    "This exercise is based on the [PyTorch 60-Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tensors\n",
    "Tensors are used to encode inputs and outputs of a model, as well as a model's parameters. They are comparable to NumPy's ndarrays, but they have the advantage that PyTorch tensors can run on GPUs and other accelerators. \n",
    "\n",
    "Extra resources: [Intuition behind tensors](https://www.youtube.com/watch?v=f5liqUk0ZTw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.1 Tensor Initialization**\n",
    "\n",
    "Tensors can be initialized in multiple ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor from data: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n",
      "Tensor from NumPy array: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n",
      "Shape given by another tensor:\n",
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.8958, 0.9213],\n",
      "        [0.7105, 0.0462]]) \n",
      "\n",
      "Shape defined: (2, 3)\n",
      "Random Tensor: \n",
      " tensor([[0.2068, 0.6002, 0.1509],\n",
      "        [0.9040, 0.2654, 0.2207]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# directly from data\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(f\"Tensor from data: \\n {x_data} \\n\")\n",
    "\n",
    "# from a NumPy array\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(f\"Tensor from NumPy array: \\n {x_np} \\n\")\n",
    "\n",
    "# from another tensor\n",
    "print(\"Shape given by another tensor:\")\n",
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "\n",
    "# with random or constant values\n",
    "shape = (2, 3,)\n",
    "print(\"Shape defined:\", shape)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.2 Tensor Attributes**\n",
    "\n",
    "We can print information such as the tensor shape, the tensor datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.3 Tensor Operations**\n",
    "\n",
    "For a full list of available tensor operations check out the corresponding [PyTorch documentation](https://pytorch.org/docs/stable/torch.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "    print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second column replaced with zeros \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# standard numpy-like indexing and slicing\n",
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(f\"Second column replaced with zeros \\n {tensor} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Indexing and Slicing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted submatrix:\n",
      " tensor([[ 5,  6,  7],\n",
      "        [ 9, 10, 11],\n",
      "        [13, 14, 15]])\n",
      "Tensor after replacement:\n",
      " tensor([[ 1,  2,  3,  4],\n",
      "        [ 0,  0,  0,  8],\n",
      "        [ 0,  0,  0, 12],\n",
      "        [ 0,  0,  0, 16]])\n"
     ]
    }
   ],
   "source": [
    "tensor_exercise = torch.tensor([[1, 2, 3, 4],\n",
    "                       [5, 6, 7, 8],\n",
    "                       [9, 10, 11, 12],\n",
    "                       [13, 14, 15, 16]])\n",
    "\n",
    "# Exercise 1: Select the last 3 rows of the first 3 columns and assign them to 'submatrix'\n",
    "submatrix = tensor_exercise[1:, :3]\n",
    "\n",
    "print(\"Extracted submatrix:\\n\", submatrix)\n",
    "\n",
    "# Exercise 2: Replace the extracted submatrix with a tensor of zeros in the original tensor\n",
    "tensor_exercise[1:, :3] = torch.zeros(submatrix.shape)\n",
    "print(\"Tensor after replacement:\\n\", tensor_exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated tensor \n",
      " tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# joining tensors (concatenation or stacking)\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(f\"Concatenated tensor \\n {t1} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# multiplying tensors\n",
    "# element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")\n",
    "\n",
    "# matrix multiplication\n",
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor before inplace addition \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "Tensor after inplace addition \n",
      " tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inplace operations have a trailing underscore\n",
    "print(f\"Tensor before inplace addition \\n {tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(f\"Tensor after inplace addition \\n {tensor} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Element-wise logarithm, multiplication and calculation of the mean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 1.3863, 3.2958, 5.5452])\n",
      "tensor(2.5568)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "# Exercise 1: Calculate the element-wise natural logarithm and multiply the result with the tensor itself\n",
    "log_tensor = tensor.log() * tensor\n",
    "print(log_tensor)\n",
    "\n",
    "# Exercise 2: Calculate the mean of the resulting tensor\n",
    "mean_log = log_tensor.mean()\n",
    "print(mean_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In-place operations can be problematic when computing derivatives because of a loss of history. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.4 Bridge with NumPy**\n",
    "\n",
    "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and a change to one will cause the other to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "# tensor to NumPy array\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "# both arrays are modified\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "n = np.ones(5)\n",
    "# NumPy array to tensor\n",
    "t = torch.from_numpy(n)\n",
    "print(f\"t: {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "# both arrays are modified\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Introduction to torch.autograd\n",
    "\n",
    "``torch.autograd`` is PyTorch's engine for automatic differentiation. It is essential for the training of neural networks.\n",
    "\n",
    "**1.2.1 Differentiation in Autograd**\n",
    "\n",
    "The argument ``required_grad=True`` signals to ``autograd`` that every operation on those tensors should be tracked. This allows ``autograd`` to collect gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If ``a`` and ``b`` are parameters of a neural networks the error function ``Q`` could looks like this:\n",
    "\n",
    "\\begin{align}Q = 3a^3 - b^2\\end{align}\n",
    "\n",
    "For the training of the neural network we need to calculate the gradients with respect to the parameters: \n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial a} = 9a^2\\end{align}\n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial b} = -2b\\end{align}\n",
    "\n",
    "With ``autograd`` you can calculate those gradients by calling ``.backward()`` on Q. The ``gradient`` argument is used here to specify how much the tensors ``a`` and ``b`` should influence the gradient calculation of ``Q``. By providing ``external_grad`` as the gradient argument, you ensure that both ``a`` and ``b`` are treated as if they contribute equally to the gradient of ``Q`` (both having a weight of 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "\n",
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **torch.autograd**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of x: tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = 2 * x[0] + 3 * x[1]\n",
    "\n",
    "# Exercise 1: Calculate gradients\n",
    "y.backward()\n",
    "\n",
    "# Exercise 2: Print gradients of 'x'\n",
    "print(f\"Gradients of x: {x.grad}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.2 Computational Graph**\n",
    "\n",
    "``autograd`` functions by maintaining a record of both data (tensors) and executed operations in a directed acyclic graph (DAG) composed of Function objects. Within this graph structure, the input tensors serve as the starting point (leaves), and the output tensors act as the endpoints (roots). By traversing this graph in a reverse manner, one can automatically compute gradients using the chain rule.\n",
    "\n",
    "- **Forward pass**: ``autograd`` performs operations to compute the resulting tensor and maintains the operation's gradient function in the DAG.\n",
    "\n",
    "- **Backward pass**: ``autograd`` (triggered by calling ``.backward()`` on the DAG root) computes the gradients from each ``.grad_fn``, accumulates them in the corresponding tensor's ``.grad`` attribute, and applies the chain rule to propagate gradients to the leaf tensors.\n",
    "\n",
    "``autograd`` tracks operations for tensors with ``requires_grad=True``, while setting ``requires_grad=False`` excludes them; if any input tensor has ``requires_grad=True``, the output tensor will also require gradients.\n",
    "\n",
    "Note: In PyTorch, DAGs (Directed Acyclic Graphs) are dynamic, and it's important to know that a new graph is built from scratch after each ``.backward()`` call. This flexibility enables the use of control flow statements and the ability to modify the model's shape, size, and operations in each iteration as required.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **DAG**\\\n",
    "Think about the answers before executing the following code. Give an explanation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients?: False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients?: {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:** ...\n",
    "1. both `x` and `y` have by default `requires_grad=False`. When they are used as input tensors to result the output tensor `a` then it would not require gradients.\n",
    "2. since `z` has the `requires_grad=True` and it acts as an input tensor for `b` therefore `b` also require gradients."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Neural Networks with PyTorch\n",
    "\n",
    "The ``torch.nn`` package can be used to construct neural networks. ``nn.Module``s contain layers, and a method ``forward(input)`` that returns the ``output``.\n",
    "\n",
    "Typical training procedure for a neural network:\n",
    "1. Define neural network with some learnable parameters (weights)\n",
    "2. Iterate over dataset of inputs\n",
    "3. Process input through network\n",
    "4. Compute the loss (how wrong is the output)\n",
    "5. Backpropagate to calculate the gradient for each of the network's weights\n",
    "6. Update the weights of the network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.1 Define the network**\n",
    "\n",
    "Define the ``forward`` function. The ``backward`` function is is automatically defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "Number of learnable parameters: 61706 \n",
      "\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "# learnable parameters of the model\n",
    "params = list(net.parameters())\n",
    "n_params = np.sum([torch.numel(p) for p in params])\n",
    "print(f\"Number of learnable parameters: {n_params} \\n\")\n",
    "print(params[0].shape)  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0324, -0.0839,  0.0366, -0.0345,  0.1424, -0.0624,  0.0044,  0.0347,\n",
      "          0.0458, -0.0400]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# random input\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero gradient buffers of all parameters and backprops with random gradients\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.2 Loss Function**\n",
    "\n",
    "The loss function computes a value that estimates how far away the output is from the target. For the full list of available loss functions check out the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html#loss-functions).\n",
    "\n",
    "Example: ``MSELoss`` (mean squared error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7415, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.3 Backprop**\n",
    "\n",
    "To initiate error backpropagation, use ``loss.backward()``, but make sure to clear existing gradients; otherwise, the new gradients will accumulate onto the existing ones. This step is crucial for accurate gradient calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0031, -0.0010, -0.0025,  0.0008, -0.0099, -0.0251])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print(\"conv1.bias.grad before backward\")\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"conv1.bias.grad after backward\")\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.4 Update the weights**\n",
    "\n",
    "A simple update rule is the Stochastic Gradient Descent (SGD):\n",
    "\n",
    "*weight = weight - learning_rate * gradient*\n",
    "\n",
    "The ``torch.optim`` package implements different update rules such as SGB, Nesterov-SGD, Adam, RMSProp, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Neural Networks Recap**\n",
    "1. What is the purpose of the forward method in a PyTorch neural network model?\n",
    "2. How do you define a convolutional layer in PyTorch, and what does the nn.Conv2d module do?\n",
    "3. In the provided code, how many learnable parameters does the neural network model Net have, and how can you access them?\n",
    "4. Explain what happens when you call net.zero_grad() and why it is important.\n",
    "5. What does the optimizer.step() method do, and when is it typically called in the training loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "1. The Forward method is used to define the Forward Pass of a neural network in PyTorch. We basically define how the input and output of each layer in our NN processes like and which activation function is used at each layer i.e, processing the whole architecture of our NN model.\n",
    "2. Convolutional layer in PyTorch is defined by defining the input in the layer, the output in the layer, and the size of the kernel (plus there are more parameters such as stride, padding, etc.). The nn.Conv2d module in PyTorch allows us to make a 2D Convolution layer.\n",
    "3. In the above Neural network model we have 61706 parameters. They are accessible using `net.parameters()`\n",
    "4. net.zero_grad() will reset all the model gradients to zero. It is important because everytime we do backpropogation in our NN model to update weights, the gradients of the previous iteration is kept. If we don't zero them out before next backpropogation iteration, the model will use accumalated value from previous iterations and hence result in incorrect training.\n",
    "5. Optimizer.step() method is used to update the weights of the NN model. It is called after the backpropogration step i.e, after loss.backward()."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Custom Layer**\\\n",
    "Create  a custom neural network layer called CustomLayer that inherits from nn.Module. The layer should take an input tensor and compute the element-wise square of the input tensor.\n",
    "1. Define the CustomLayer class with an init and a forward method that performs the specified operation.\n",
    "2. Instantiate the CustomLayer.\n",
    "3. Generate a random input tensor with dimensions (1, 3, 4, 4).\n",
    "4. Apply the CustomLayer to the input tensor to calculate the element-wise square.\n",
    "5. Print the input tensor and the resulting output tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor: tensor([[[[ 0.4842, -0.3763,  1.7015,  0.2871],\n",
      "          [ 0.3583, -0.2179,  0.0930, -0.7382],\n",
      "          [ 0.1237, -0.5222, -0.5866,  0.9115],\n",
      "          [ 2.5524,  1.4754,  1.2114,  0.0345]],\n",
      "\n",
      "         [[-0.0429,  0.3397,  0.7697,  0.4871],\n",
      "          [ 0.4227, -0.8150, -0.5031, -0.0501],\n",
      "          [-0.5136, -0.1476, -2.0534, -1.1029],\n",
      "          [ 0.3540, -0.1150, -1.1436,  0.9438]],\n",
      "\n",
      "         [[-1.2858,  0.3421,  0.5400,  0.2203],\n",
      "          [ 0.2352, -1.5010, -0.5221, -0.4569],\n",
      "          [-1.3639,  0.6557,  2.2823,  2.6140],\n",
      "          [ 0.8385, -0.0738,  0.8399, -1.1682]]]])\n",
      "\n",
      "Output Tensor (Element-wise Square): tensor([[[[2.3446e-01, 1.4162e-01, 2.8952e+00, 8.2428e-02],\n",
      "          [1.2837e-01, 4.7472e-02, 8.6493e-03, 5.4493e-01],\n",
      "          [1.5304e-02, 2.7269e-01, 3.4416e-01, 8.3083e-01],\n",
      "          [6.5149e+00, 2.1768e+00, 1.4675e+00, 1.1925e-03]],\n",
      "\n",
      "         [[1.8431e-03, 1.1540e-01, 5.9243e-01, 2.3729e-01],\n",
      "          [1.7868e-01, 6.6415e-01, 2.5309e-01, 2.5097e-03],\n",
      "          [2.6374e-01, 2.1790e-02, 4.2166e+00, 1.2163e+00],\n",
      "          [1.2528e-01, 1.3231e-02, 1.3079e+00, 8.9078e-01]],\n",
      "\n",
      "         [[1.6532e+00, 1.1706e-01, 2.9156e-01, 4.8539e-02],\n",
      "          [5.5334e-02, 2.2530e+00, 2.7258e-01, 2.0879e-01],\n",
      "          [1.8601e+00, 4.2989e-01, 5.2090e+00, 6.8330e+00],\n",
      "          [7.0310e-01, 5.4478e-03, 7.0543e-01, 1.3647e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Define the CustomLayer class that performs and outputs the elementwise square function on the inputs\n",
    "class CustomLayer(nn.Module):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(CustomLayer, self).__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x ** 2\n",
    "\n",
    "\n",
    "# 2. Instantiate the CustomLayer\n",
    "net = CustomLayer()\n",
    "\n",
    "# 3. Generate a random input tensor\n",
    "input = torch.randn(1, 3, 4, 4)\n",
    "\n",
    "# 4. Apply the CustomLayer to the input tensor\n",
    "output = net.forward(input)\n",
    "\n",
    "# 5. Print the input tensor and the resulting output tensor\n",
    "print(\"Input Tensor:\", input)\n",
    "# print(input_tensor)\n",
    "\n",
    "print(\"\\nOutput Tensor (Element-wise Square):\", output)\n",
    "# print(output_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Behavior Cloning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name implies, Behavior Cloning is a machine learning technique where an **agent** learns to mimic the behavior of an **expert**. Hence, we (the agent) aim to clone the behavior of another agent which already peforms well on the task at hand. For a better intuition, we recommend this [Introduction into Behavior Cloning](https://www.youtube.com/watch?v=GIxDtuaC3To). Our task is the [MinAtar Breakout](https://github.com/kenjyoung/MinAtar), a game where a player controls a paddle to bounce a ball and break bricks.\n",
    "\n",
    "Therefore, we provide expert behavior in form of observation-acion pairs retrived by a well-trained policy. Your exercise will be to implement and train a simple neural network in PyTorch, that is able to aimitate the behavior of the expert and perform well on the task.\n",
    "\n",
    "We do the following steps:\n",
    "1. Load the Minatar breakout datasets\n",
    "2. Define a Convolutional Neural Network (CNN)\n",
    "3. Define a loss function\n",
    "4. Train the network on training data\n",
    "5. Test the network on test data\n",
    "\n",
    "\n",
    "<img src=\"breakout.gif\" alt=\"MinAtar\" width=\"20%\"/>\n",
    "\n",
    "_Agent using random actions to play MinAtar Breakout_  \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Loading data**\n",
    "\n",
    "When working with image, text, audio, or video data, you can use standard Python packages to load the data into a NumPy array, which can then be transformed into a ``torch.*Tensor``. However, for large datasets, computing gradients for all data points simultaneously isn't feasible. Therefore, we adopt a technique called **batching**, where the neural network is fed chunks of our data at a time. PyTorch offers the DataLoader class for efficient management of batching and data loading. By providing the ``DataLoader`` with the data and batch size, we obtain an iterable object that can be used for training or testing. It's a good practice to shuffle the data inside the loader to reduce biases during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 100000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load observations and actions\n",
    "with open('dataset.npy', 'rb') as f:\n",
    "    obss = np.load(f)\n",
    "    acts = np.load(f)\n",
    "\n",
    "# Convert to torch tensors\n",
    "obss = torch.from_numpy(obss).float() # PyTorch requires float or double dtype, not bool\n",
    "acts = torch.from_numpy(acts)\n",
    "\n",
    "# Create dataset and dataloaders, which we can iterate over\n",
    "dataset = []\n",
    "for i in range(len(obss)):\n",
    "    dataset.append((obss[i], acts[i]))\n",
    "\n",
    "# Do a 80% / 20% split between train and test set\n",
    "train_loader = DataLoader(dataset[:int(4*len(obss)/5)], batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(dataset[int(4*len(obss)/5):], batch_size=128, shuffle=True)\n",
    "\n",
    "print(f\"Size of dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we take a look at the samples of the dataset. Each sample consists of an observation paired with the corresponding action our agent should take. The observation is an image capturing the current game frame encoded as a NumPy array. As for the actions, they are represented by integers, where 0 describes \"DO NOTHING,\" 1 = \"MOVE LEFT,\" and 2 = \"MOVE RIGHT\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation is represented as Tensor with shape of torch.Size([10, 10, 4]). The first and second dimensions correspond to the height and width, while the third dimension describes the different game objects.\n",
      "Additionally, the data type (dtype) of the observation is torch.float32. Each value in the array indicates whether a particular object is present (1.0 = True) or not present (0.0 = False).\n",
      "The current action is 1 = MOVE LEFT\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get some arbitrary sample\n",
    "obs, act = dataset[10]\n",
    "\n",
    "# Actions and their meaning\n",
    "ACTIONS = {\n",
    "    0: \"DO NOTHING\",\n",
    "    1: \"MOVE LEFT\",\n",
    "    2: \"MOVE RIGHT\",\n",
    "}\n",
    "\n",
    "def to_rgb(obs):\n",
    "    \"\"\" Converts the observation into an rgb image. Taken from MinAtar. \"\"\"\n",
    "    obs = obs.bool().numpy()\n",
    "    n_channels = obs.shape[-1]\n",
    "    cmap = sns.color_palette(\"cubehelix\", n_channels)\n",
    "    cmap.insert(0, (0,0,0))\n",
    "    numerical_state = np.amax(obs * np.reshape(np.arange(n_channels) + 1, (1,1,-1)), 2)\n",
    "    rgb_array = np.stack(cmap)[numerical_state]\n",
    "    return rgb_array\n",
    "\n",
    "print(f\"The observation is represented as {obs.__class__.__name__} with shape of {obs.shape}. The first and second dimensions correspond to the height and width, while the third dimension describes the different game objects.\")\n",
    "print(f\"Additionally, the data type (dtype) of the observation is {obs.dtype}. Each value in the array indicates whether a particular object is present (1.0 = True) or not present (0.0 = False).\")\n",
    "print(f\"The current action is {act} = {ACTIONS[act.item()]}\")\n",
    "\n",
    "# Render observation\n",
    "img = to_rgb(obs)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Define Convolutional Neural Network**\n",
    "\n",
    "Here we define our own classifier that takes as input a batch of observations, and ouputs three logits for each observation. These logits can be fed into a softmax function later to obtain probabilities for the actions. However, it is more convenient for the ``CrossEntropyLoss`` used in classifiers to retrieve the raw logits. We want our model to have two convolutional layers followed by two linear layers. Each but the last layer should be followed by a Rectified Linear Unit (ReLU) activation function.\n",
    "\n",
    "Build a neural network, similar as in 1.3. Notice: it has to take as input 4-channel images. Also, we want the convolutional layers to output the same image height and width and therefore use a stride of one. However, because the kernel size is >1, padding around the images is needed. Visualize to yourself how the kernel size is leading to a change of output size and what padding is needed to keep the dimensions constant. See the PyTorch documentation for convolutional layers to get hints for the padding argument.\n",
    "\n",
    "<img src=\"padding.gif\" alt=\"MinAtar\" width=\"50%\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Defining a CNN**\n",
    "\n",
    "Fill in the gaps in the following code.\n",
    "\n",
    "1. Define the first convolutional layer with 4 input channels (given by the data) and 16 output channels, stride 1, and kernel size 3.\n",
    "    \n",
    "    Tipp: Apply padding, such that the image dimensions stay the same\n",
    "\n",
    "2. Define the second convolutional layer with 32 output channels, stride 1, and kernel size 3.\n",
    "\n",
    "    Tipp: Apply padding, such that the image dimensions stay the same\n",
    "\n",
    "3. Define the first linear layer mapping from the flattened output of convolutional layer 2, to 128 output nodes.\n",
    "\n",
    "    Tipp: How can the input nodes be retrieved by the ouput channels of convolutional layer 2 and the input data shape?\n",
    "\n",
    "4. Define the last linear layer\n",
    "\n",
    "    Tipp: How many output nodes do we need given the action space?\n",
    "\n",
    "5. Define the forward pass function by putting all the layers together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, obs_shape):\n",
    "        # obs_shape is the shape of a single observation -> use this information to define the dimensions of the layers\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=32 * obs_shape[0] * obs_shape[1], out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=3)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2) # CNN format (batch_size, channels, height, width)\n",
    "\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "net = CNN(obs_shape=obss.shape[1:])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net.to('cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Define loss function and optimizer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Defining a loss function**\\\n",
    "Implement a cross entropy loss function\n",
    "\n",
    "Hint: check out the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Defining an optimizer**\\\n",
    "Implement an adam optimizer and pass the learning rate as argument\n",
    "\n",
    "Hint: check out the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Train network**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Training the CNN**\\\n",
    "Fill in the gaps in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist = []\n",
    "acc_hist = []\n",
    "\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        # perform the forward pass\n",
    "        outputs = ...\n",
    "\n",
    "        # calculate the loss ussing the criterion\n",
    "        loss = ...\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        ...\n",
    "\n",
    "        # perform the backward pass to compute gradients\n",
    "        ...\n",
    "\n",
    "        # update the model's parameters using the optimizer\n",
    "        ...\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'{epoch + 1} | loss: {running_loss / total:.6f} | accuracy:  {correct/total:.2f}')\n",
    "\n",
    "    loss_hist.append(running_loss / total)\n",
    "    acc_hist.append(correct/total)\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Plotting the training results**\\\n",
    "Plot the accuracy and loss over training steps. Make sure to give meaningful axes labels. If you are not familiar with ``matplotlib``, see this [quick start guide](https://matplotlib.org/stable/users/explain/quick_start.html#quick-start)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(8,3))\n",
    "\n",
    "# plot accuracy over epochs\n",
    "...\n",
    "\n",
    "# plot loss over epochs\n",
    "...\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Interpreting training results**\n",
    "1. Did the training converge? Why do you think so?\n",
    "2. After how many epochs could we have stopped the training?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5 Test network on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test observations: {100 * correct // total} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performance is significantly better than random chance, which would be a 33% accuracy when randomly selecting from 3 classes. It appears that the network has learned something meaningful.\n",
    "\n",
    "Lastly, we want to create a short GIF showing our agent playing the breakout task. You don't need to understand the details of how this function works yet. Notice: It saves each game frame as breakout_minatar.png and removes the file in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "import gymnasium as gym\n",
    "from matplotlib import colors\n",
    "\n",
    "\n",
    "def create_gif():\n",
    "    \"\"\" Creates a GIF of the agent playing in the current folder. \"\"\"#\n",
    "    # Create environment and reset\n",
    "    env = gym.make('MinAtar/Breakout-v1')\n",
    "    obs, info = env.reset()\n",
    "    images = []\n",
    "\n",
    "    for i in range(1_000):\n",
    "        # Get action\n",
    "        obs = torch.tensor(obs, dtype=torch.float).unsqueeze(0)\n",
    "        if torch.cuda.is_available():\n",
    "            obs = obs.cuda()\n",
    "        act_probs = net(obs)\n",
    "        act = np.argmax(act_probs.detach().cpu().numpy())\n",
    "\n",
    "        # Step the environment\n",
    "        obs, reward, terminated, truncated, info = env.step(act)\n",
    "\n",
    "        # Render as image\n",
    "        n_channels = 4\n",
    "        obs = obs.astype(np.float32)\n",
    "        cmap = sns.color_palette(\"cubehelix\", n_channels)\n",
    "        cmap.insert(0, (0,0,0))\n",
    "        cmap = colors.ListedColormap(cmap)\n",
    "        numerical_state = np.amax(obs * np.reshape(np.arange(n_channels) + 1, (1,1,-1)), 2) + 0.5\n",
    "        numerical_state = numerical_state.repeat(48, axis=0).repeat(48, axis=1)\n",
    "        plt.imsave('breakout_minatar.png', numerical_state, cmap=cmap)\n",
    "        images.append(imageio.imread('breakout_minatar.png'))\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # Save GIF\n",
    "    imageio.mimsave('breakout.gif', images, loop=0, fps=20)\n",
    "    os.remove('breakout_minatar.png')\n",
    "    env.close()\n",
    "\n",
    "create_gif()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6 Additional questions**\n",
    "\n",
    "So far, the hyperparameters for the training were given. We now want to investigate how different hyperparameters influence the training results. Additionally, there are many techniques and architectures to improve robustness and efficiency of neural network training. It is time for you to explore and find a few more.\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Ablation studies: Effect of hyperparameters**\n",
    "1. Learning rate: Test different learning rates of the Adam Optimizer. What happens if the learaning rate is too large/too small?\n",
    "2. Epochs: Can you think of any drawbacks of training a network for too many epochs? If yes, how can we adjust the training to cope for these drawbacks?\n",
    "3. Number of latent channels: Try different values for the number of latent channels. What happens if you increase/decrease the number of channels? Does an increase of channels increase the paramters of a convolutional layer or linear layer more and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Robustness and efficiency**\n",
    "1. We investigated the effect of layer channels. What other way can you think of to make a neural net more complex/expressive than increasing the channel sizes?\n",
    "2. What is underfitting/overfitting? Explain in a few short sentences.\n",
    "3. Name one technique to prevent underfitting, and one to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:...**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
