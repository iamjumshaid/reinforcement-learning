{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Introduction to PyTorch and Behavior Cloning\n",
    "\n",
    "In this exercise, we introduce the fundamental concepts of PyTorch and show how this framework can be applied to build and train custom neural networks. \n",
    "We then utilize these networks to train an agent to play a simple video game from demonstrations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Install packages and dependencies\n",
    "\n",
    "We recommend using a ``conda`` environment (or virtual environments) for this and the follwing exercises. If you are not familiar with Anaconda, you can read this tutorial: [Getting started with conda]( https://conda.io/projects/conda/en/latest/user-guide/getting-started.html). \n",
    "\n",
    "Here's an example of the steps you can take:\n",
    "1. Install a conda distribution on your machine\n",
    "2. Create a new conda Python 3.9 environment with name rl_lab: ``conda create --name rl_lab python=3.9``\n",
    "3. Activate the environment: ``conda activate rl_lab``\n",
    "4. Install dependencies using the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/brosseit/.local/lib/python3.10/site-packages (2.0.1+cu117)\n",
      "Requirement already satisfied: torchvision in /home/brosseit/.local/lib/python3.10/site-packages (0.15.2+cu117)\n",
      "Requirement already satisfied: torchaudio in /home/brosseit/.local/lib/python3.10/site-packages (2.0.2+cu117)\n",
      "Requirement already satisfied: filelock in /home/brosseit/.local/lib/python3.10/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/brosseit/anaconda3/envs/rl/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/brosseit/.local/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/brosseit/.local/lib/python3.10/site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /home/brosseit/.local/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/brosseit/.local/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: cmake in /home/brosseit/.local/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.25.0)\n",
      "Requirement already satisfied: lit in /home/brosseit/.local/lib/python3.10/site-packages (from triton==2.0.0->torch) (15.0.7)\n",
      "Requirement already satisfied: numpy in /home/brosseit/.local/lib/python3.10/site-packages (from torchvision) (1.24.1)\n",
      "Requirement already satisfied: requests in /home/brosseit/.local/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/brosseit/.local/lib/python3.10/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/brosseit/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/brosseit/.local/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/brosseit/anaconda3/envs/rl/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/brosseit/.local/lib/python3.10/site-packages (from requests->torchvision) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/brosseit/anaconda3/envs/rl/lib/python3.10/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/brosseit/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: gymnasium==0.29.1 in /home/brosseit/.local/lib/python3.10/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/brosseit/.local/lib/python3.10/site-packages (from gymnasium==0.29.1) (1.24.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/brosseit/.local/lib/python3.10/site-packages (from gymnasium==0.29.1) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/brosseit/anaconda3/envs/rl/lib/python3.10/site-packages (from gymnasium==0.29.1) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/brosseit/.local/lib/python3.10/site-packages (from gymnasium==0.29.1) (0.0.4)\n",
      "Requirement already satisfied: minatar==1.0.15 in /home/brosseit/.local/lib/python3.10/site-packages (1.0.15)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /home/brosseit/.local/lib/python3.10/site-packages (from minatar==1.0.15) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/brosseit/.local/lib/python3.10/site-packages (from minatar==1.0.15) (1.4.5)\n",
      "Requirement already satisfied: matplotlib>=3.0.3 in /home/brosseit/.local/lib/python3.10/site-packages (from minatar==1.0.15) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /home/brosseit/.local/lib/python3.10/site-packages (from minatar==1.0.15) (1.24.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /home/brosseit/.local/lib/python3.10/site-packages (from minatar==1.0.15) (2.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/brosseit/.local/lib/python3.10/site-packages (from minatar==1.0.15) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /home/brosseit/.local/lib/python3.10/site-packages (from minatar==1.0.15) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2018.9 in /home/brosseit/.local/lib/python3.10/site-packages (from minatar==1.0.15) (2023.3.post1)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /home/brosseit/.local/lib/python3.10/site-packages (from minatar==1.0.15) (1.11.3)\n",
      "Requirement already satisfied: seaborn>=0.9.0 in /home/brosseit/.local/lib/python3.10/site-packages (from minatar==1.0.15) (0.13.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/brosseit/.local/lib/python3.10/site-packages (from minatar==1.0.15) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/brosseit/.local/lib/python3.10/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (1.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/brosseit/.local/lib/python3.10/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (4.43.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/brosseit/.local/lib/python3.10/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/brosseit/.local/lib/python3.10/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (10.1.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/brosseit/.local/lib/python3.10/site-packages (from pandas>=0.24.2->minatar==1.0.15) (2023.3)\n",
      "Requirement already satisfied: matplotlib in /home/brosseit/.local/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/brosseit/.local/lib/python3.10/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/brosseit/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/brosseit/.local/lib/python3.10/site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/brosseit/.local/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.19 in /home/brosseit/.local/lib/python3.10/site-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/brosseit/.local/lib/python3.10/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/brosseit/.local/lib/python3.10/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/brosseit/.local/lib/python3.10/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/brosseit/.local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/brosseit/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: imageio in /home/brosseit/.local/lib/python3.10/site-packages (2.32.0)\n",
      "Requirement already satisfied: numpy in /home/brosseit/.local/lib/python3.10/site-packages (from imageio) (1.24.1)\n",
      "Collecting pillow<10.1.0,>=8.3.2 (from imageio)\n",
      "  Downloading Pillow-10.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Downloading Pillow-10.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pillow\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 10.1.0\n",
      "    Uninstalling Pillow-10.1.0:\n",
      "      Successfully uninstalled Pillow-10.1.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jumanji 0.3.1 requires gym>=0.22.0, but you have gym 0.21.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pillow-10.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install gymnasium==0.29.1\n",
    "!pip install minatar==1.0.15\n",
    "!pip install matplotlib\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction to PyTorch \n",
    "\n",
    "Total: 10 pts\n",
    "\n",
    "PyTorch is a Python-based library designed for deep learning. It is distinguished by its dynamic computational graph, which enables researchers and developers to construct models with a high degree of flexibility. PyTorch has found extensive use in various scientific and engineering domains due to its ease of use and extensive research-friendly features.\n",
    "\n",
    "This exercise is based on the [PyTorch 60-Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tensors\n",
    "Tensors are used to encode inputs and outputs of a model, as well as a model's parameters. They are comparable to NumPy's ndarrays, but they have the advantage that PyTorch tensors can run on GPUs and other accelerators. \n",
    "\n",
    "Extra resources: [Intuition behind tensors](https://www.youtube.com/watch?v=f5liqUk0ZTw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.1 Tensor Initialization**\n",
    "\n",
    "Tensors can be initialized in multiple ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor from data: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n",
      "Tensor from NumPy array: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n",
      "Shape given by another tensor:\n",
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.4467, 0.2327],\n",
      "        [0.7722, 0.3409]]) \n",
      "\n",
      "Shape defined: (2, 3)\n",
      "Random Tensor: \n",
      " tensor([[0.9252, 0.3681, 0.5175],\n",
      "        [0.5615, 0.6293, 0.5842]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# directly from data\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(f\"Tensor from data: \\n {x_data} \\n\")\n",
    "\n",
    "# from a NumPy array\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(f\"Tensor from NumPy array: \\n {x_np} \\n\")\n",
    "\n",
    "# from another tensor\n",
    "print(\"Shape given by another tensor:\")\n",
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "\n",
    "# with random or constant values\n",
    "shape = (2, 3,)\n",
    "print(\"Shape defined:\", shape)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.2 Tensor Attributes**\n",
    "\n",
    "We can print information such as the tensor shape, the tensor datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.3 Tensor Operations**\n",
    "\n",
    "For a full list of available tensor operations check out the corresponding [PyTorch documentation](https://pytorch.org/docs/stable/torch.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tensor is stored on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Moving tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "    print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second column replaced with zeros \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# standard numpy-like indexing and slicing\n",
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(f\"Second column replaced with zeros \\n {tensor} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Indexing and Slicing**  (0.5 + 0.5 = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted submatrix:\n",
      " tensor([[ 5,  6,  7],\n",
      "        [ 9, 10, 11],\n",
      "        [13, 14, 15]])\n",
      "Tensor after replacement:\n",
      " tensor([[ 1,  2,  3,  4],\n",
      "        [ 0,  0,  0,  8],\n",
      "        [ 0,  0,  0, 12],\n",
      "        [ 0,  0,  0, 16]])\n"
     ]
    }
   ],
   "source": [
    "tensor_exercise = torch.tensor([[1, 2, 3, 4],\n",
    "                       [5, 6, 7, 8],\n",
    "                       [9, 10, 11, 12],\n",
    "                       [13, 14, 15, 16]])\n",
    "\n",
    "# Exercise 1: Select the last 3 rows of the first 3 columns and assign them to 'submatrix'\n",
    "submatrix = tensor_exercise[-3:,:3] \n",
    "print(\"Extracted submatrix:\\n\", submatrix)\n",
    "\n",
    "# Exercise 2: Replace the extracted submatrix with a tensor of zeros in the original tensor\n",
    "tensor_exercise[-3:,:3] = torch.zeros_like(submatrix)\n",
    "print(\"Tensor after replacement:\\n\", tensor_exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated tensor \n",
      " tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# joining tensors (concatenation or stacking)\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(f\"Concatenated tensor \\n {t1} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# multiplying tensors\n",
    "# element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")\n",
    "\n",
    "# matrix multiplication\n",
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor before inplace addition \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "Tensor after inplace addition \n",
      " tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inplace operations have a trailing underscore\n",
    "print(f\"Tensor before inplace addition \\n {tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(f\"Tensor after inplace addition \\n {tensor} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Element-wise logarithm, multiplication and calculation of the mean**  (0.5 + 0.5 + 0.5 = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 1.3863, 3.2958, 5.5452])\n",
      "tensor(2.5568)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "# Exercise 1: Calculate the element-wise natural logarithm and multiply the result with the tensor itself\n",
    "log_tensor = tensor.log()\n",
    "log_tensor = log_tensor.mul(tensor)\n",
    "print(log_tensor)\n",
    "\n",
    "# Exercise 2: Calculate the mean of the resulting tensor\n",
    "mean_log = log_tensor.mean()\n",
    "print(mean_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In-place operations can be problematic when computing derivatives because of a loss of history. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.4 Bridge with NumPy**\n",
    "\n",
    "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and a change to one will cause the other to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "# tensor to NumPy array\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "# both arrays are modified\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "# NumPy array to tensor\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "# both arrays are modified\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Introduction to torch.autograd\n",
    "\n",
    "``torch.autograd`` is PyTorch's engine for automatic differentiation. It is essential for the training of neural networks.\n",
    "\n",
    "**1.2.1 Differentiation in Autograd**\n",
    "\n",
    "The argument ``required_grad=True`` signals to ``autograd`` that every operation on those tensors should be tracked. This allows ``autograd`` to collect gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If ``a`` and ``b`` are parameters of a neural networks the error function ``Q`` could looks like this:\n",
    "\n",
    "\\begin{align}Q = 3a^3 - b^2\\end{align}\n",
    "\n",
    "For the training of the neural network we need to calculate the gradients with respect to the parameters: \n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial a} = 9a^2\\end{align}\n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial b} = -2b\\end{align}\n",
    "\n",
    "With ``autograd`` you can calculate those gradients by calling ``.backward()`` on Q. The ``gradient`` argument is used here to specify how much the tensors ``a`` and ``b`` should influence the gradient calculation of ``Q``. By providing ``external_grad`` as the gradient argument, you ensure that both ``a`` and ``b`` are treated as if they contribute equally to the gradient of ``Q`` (both having a weight of 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "\n",
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **torch.autograd** (0.5 + 0.5 = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of x: tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = 2 * x[0] + 3 * x[1]\n",
    "\n",
    "# Exercise 1: Calculate gradients\n",
    "y.backward()\n",
    "\n",
    "# Exercise 2: Print gradients of 'x'\n",
    "print(\"Gradients of x:\", x.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.2 Computational Graph**\n",
    "\n",
    "``autograd`` functions by maintaining a record of both data (tensors) and executed operations in a directed acyclic graph (DAG) composed of Function objects. Within this graph structure, the input tensors serve as the starting point (leaves), and the output tensors act as the endpoints (roots). By traversing this graph in a reverse manner, one can automatically compute gradients using the chain rule.\n",
    "\n",
    "- **Forward pass**: ``autograd`` performs operations to compute the resulting tensor and maintains the operation's gradient function in the DAG.\n",
    "\n",
    "- **Backward pass**: ``autograd`` (triggered by calling ``.backward()`` on the DAG root) computes the gradients from each ``.grad_fn``, accumulates them in the corresponding tensor's ``.grad`` attribute, and applies the chain rule to propagate gradients to the leaf tensors.\n",
    "\n",
    "``autograd`` tracks operations for tensors with ``requires_grad=True``, while setting ``requires_grad=False`` excludes them; if any input tensor has ``requires_grad=True``, the output tensor will also require gradients.\n",
    "\n",
    "Note: In PyTorch, DAGs (Directed Acyclic Graphs) are dynamic, and it's important to know that a new graph is built from scratch after each ``.backward()`` call. This flexibility enables the use of control flow statements and the ability to modify the model's shape, size, and operations in each iteration as required.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **DAG**\\\n",
    "Think about the answers before executing the following code. Give an explanation. (0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients?: False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients?: {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:** If at least one leaf in the graph requires grad, the root also requires grad. This is the case for tensor b, but not for a."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Neural Networks with PyTorch\n",
    "\n",
    "The ``torch.nn`` package can be used to construct neural networks. ``nn.Module``s contain layers, and a method ``forward(input)`` that returns the ``output``.\n",
    "\n",
    "Typical training procedure for a neural network:\n",
    "1. Define neural network with some learnable parameters (weights)\n",
    "2. Iterate over dataset of inputs\n",
    "3. Process input through network\n",
    "4. Compute the loss (how wrong is the output)\n",
    "5. Backpropagate to calculate the gradient for each of the network's weights\n",
    "6. Update the weights of the network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.1 Define the network**\n",
    "\n",
    "Define the ``forward`` function. The ``backward`` function is is automatically defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "Number of learnable parameters: 61706 \n",
      "\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "# learnable parameters of the model\n",
    "params = list(net.parameters())\n",
    "n_params = np.sum([torch.numel(p) for p in params])\n",
    "print(f\"Number of learnable parameters: {n_params} \\n\") \n",
    "print(params[0].shape)  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1076,  0.1641,  0.0884, -0.0909,  0.0652,  0.0816,  0.1704, -0.0081,\n",
      "          0.2238,  0.0687]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# random input\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero gradient buffers of all parameters and backprops with random gradients\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.2 Loss Function**\n",
    "\n",
    "The loss function computes a value that estimates how far away the output is from the target. For the full list of available loss functions check out the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html#loss-functions).\n",
    "\n",
    "Example: ``MSELoss`` (mean squared error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9356, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.3 Backprop**\n",
    "\n",
    "To initiate error backpropagation, use ``loss.backward()``, but make sure to clear existing gradients; otherwise, the new gradients will accumulate onto the existing ones. This step is crucial for accurate gradient calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0224,  0.0063,  0.0061,  0.0295,  0.0039, -0.0003])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print(\"conv1.bias.grad before backward\")\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"conv1.bias.grad after backward\")\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.4 Update the weights**\n",
    "\n",
    "A simple update rule is the Stochastic Gradient Descent (SGD):\n",
    "\n",
    "*weight = weight - learning_rate * gradient*\n",
    "\n",
    "The ``torch.optim`` package implements different update rules such as SGB, Nesterov-SGD, Adam, RMSProp, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Neural Networks Recap**\n",
    "1. What is the purpose of the forward method in a PyTorch neural network model? (0.5)\n",
    "2. How do you define a convolutional layer in PyTorch, and what does the nn.Conv2d module do? (1)\n",
    "3. In the provided code, how many learnable parameters does the neural network model Net have, and how can you access them? (0.5)\n",
    "4. Explain what happens when you call net.zero_grad() and why it is important. (0.5)\n",
    "5. What does the optimizer.step() method do, and when is it typically called in the training loop? (0.5)\n",
    "\n",
    "(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:** \n",
    "1. The forward method passes the inputs through the nn layers to produce an output. Hereby, the DAG is built internally.\n",
    "2. With the nn module nn.Conv2d(), and by setting the channel sizes, stride length, padding, and the kernel size. The concolutional layer runs filters over the input by performing a strided convolution operation. Hereby, the weights of the filters are shared, making the convolutional layers very memory efficient.\n",
    "3. It has 61706 parameters. A list of parameters of each layer can be accesed with the net.parameters() function.\n",
    "4. net.zero_grad() zeros all gradients in the network. This ensures, that gradients are not accumulated over multiple operations or optimization steps.\n",
    "5. optimizer.step() makes one update of all learnable parameters. It is usually called after the a forward and a backward pass of a training batch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Custom Layer**\\\n",
    "Create  a custom neural network layer called CustomLayer that inherits from nn.Module. The layer should take an input tensor and compute the element-wise square of the input tensor.\n",
    "1. Define the CustomLayer class with an init and a forward method that performs the specified operation. (1)\n",
    "2. Instantiate the CustomLayer. (0.5)\n",
    "3. Generate a random input tensor with dimensions (1, 3, 4, 4). (0.5)\n",
    "4. Apply the CustomLayer to the input tensor to calculate the element-wise square. (0.5)\n",
    "5. Print the input tensor and the resulting output tensor. (0.5)\n",
    "\n",
    "(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor: tensor([[[[ 0.3643,  1.1265,  0.0502, -0.7128],\n",
      "          [-1.1624,  2.0566, -1.5815, -0.7854],\n",
      "          [-1.4862, -1.0527,  0.8001, -0.9875],\n",
      "          [-0.4303,  0.1302, -0.1723, -1.0561]],\n",
      "\n",
      "         [[-0.4151,  1.9966,  0.9825, -1.0531],\n",
      "          [-0.4854, -1.5169, -0.8515, -0.1764],\n",
      "          [-1.7115, -0.8083, -0.7079,  0.6193],\n",
      "          [ 0.6462,  0.2917, -0.0091,  2.4190]],\n",
      "\n",
      "         [[-0.3252, -0.3180, -0.1790,  1.1481],\n",
      "          [ 2.8049,  0.8472, -0.6853, -2.1451],\n",
      "          [-0.7011,  1.8987, -0.0527, -2.1506],\n",
      "          [ 1.1021,  1.2414,  1.0250,  1.3823]]]])\n",
      "\n",
      "Output Tensor (Element-wise Square): tensor([[[[1.3268e-01, 1.2690e+00, 2.5221e-03, 5.0815e-01],\n",
      "          [1.3512e+00, 4.2296e+00, 2.5013e+00, 6.1685e-01],\n",
      "          [2.2086e+00, 1.1081e+00, 6.4009e-01, 9.7511e-01],\n",
      "          [1.8519e-01, 1.6960e-02, 2.9688e-02, 1.1154e+00]],\n",
      "\n",
      "         [[1.7234e-01, 3.9864e+00, 9.6526e-01, 1.1091e+00],\n",
      "          [2.3562e-01, 2.3009e+00, 7.2511e-01, 3.1111e-02],\n",
      "          [2.9291e+00, 6.5331e-01, 5.0118e-01, 3.8356e-01],\n",
      "          [4.1758e-01, 8.5083e-02, 8.2893e-05, 5.8513e+00]],\n",
      "\n",
      "         [[1.0577e-01, 1.0114e-01, 3.2057e-02, 1.3181e+00],\n",
      "          [7.8673e+00, 7.1775e-01, 4.6962e-01, 4.6013e+00],\n",
      "          [4.9159e-01, 3.6049e+00, 2.7795e-03, 4.6250e+00],\n",
      "          [1.2145e+00, 1.5412e+00, 1.0506e+00, 1.9106e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Define the CustomLayer class that performs and outputs the elementwise square function on the inputs\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x**2\n",
    "\n",
    "# 2. Instantiate the CustomLayer\n",
    "cl = CustomLayer()\n",
    "\n",
    "# 3. Generate a random input tensor\n",
    "input = torch.randn(1,3,4,4)\n",
    "\n",
    "# 4. Apply the CustomLayer to the input tensor\n",
    "output = cl(input)\n",
    "# 5. Print the input tensor and the resulting output tensor\n",
    "print(\"Input Tensor:\", input)\n",
    "# print(input_tensor)\n",
    "print(\"\\nOutput Tensor (Element-wise Square):\", output)\n",
    "# print(output_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Behavior Cloning\n",
    "\n",
    "Total: 16 pts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name implies, Behavior Cloning is a machine learning technique where an **agent** learns to mimic the behavior of an **expert**. Hence, we (the agent) aim to clone the behavior of another agent which already peforms well on the task at hand. For a better intuition, we recommend this [Introduction into Behavior Cloning](https://www.youtube.com/watch?v=GIxDtuaC3To). Our task is the [MinAtar Breakout](https://github.com/kenjyoung/MinAtar), a game where a player controls a paddle to bounce a ball and break bricks.\n",
    "\n",
    "Therefore, we provide expert behavior in form of observation-acion pairs retrived by a well-trained policy. Your exercise will be to implement and train a simple neural network in PyTorch, that is able to aimitate the behavior of the expert and perform well on the task.\n",
    "\n",
    "We do the following steps:\n",
    "1. Load the Minatar breakout datasets\n",
    "2. Define a Convolutional Neural Network (CNN)\n",
    "3. Define a loss function\n",
    "4. Train the network on training data\n",
    "5. Test the network on test data\n",
    "\n",
    "\n",
    "<img src=\"breakout.gif\" alt=\"MinAtar\" width=\"20%\"/>\n",
    "\n",
    "_Agent using random actions to play MinAtar Breakout_  \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Loading data**\n",
    "\n",
    "When working with image, text, audio, or video data, you can use standard Python packages to load the data into a NumPy array, which can then be transformed into a ``torch.*Tensor``. However, for large datasets, computing gradients for all data points simultaneously isn't feasible. Therefore, we adopt a technique called **batching**, where the neural network is fed chunks of our data at a time. PyTorch offers the DataLoader class for efficient management of batching and data loading. By providing the ``DataLoader`` with the data and batch size, we obtain an iterable object that can be used for training or testing. It's a good practice to shuffle the data inside the loader to reduce biases during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 100000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load observations and actions\n",
    "with open('resources/dataset.npy', 'rb') as f:\n",
    "    obss = np.load(f)\n",
    "    acts = np.load(f)\n",
    "\n",
    "# Convert to torch tensors\n",
    "obss = torch.from_numpy(obss).float() # PyTorch requires float or double dtype, not bool\n",
    "acts = torch.from_numpy(acts)\n",
    "\n",
    "# Create dataset and dataloaders, which we can iterate over\n",
    "dataset = []\n",
    "for i in range(len(obss)):\n",
    "    dataset.append((obss[i], acts[i]))\n",
    "\n",
    "# Do a 80% / 20% split between train and test set\n",
    "train_loader = DataLoader(dataset[:int(4*len(obss)/5)], batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(dataset[int(4*len(obss)/5):], batch_size=128, shuffle=True)\n",
    "\n",
    "print(f\"Size of dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we take a look at the samples of the dataset. Each sample consists of an observation paired with the corresponding action our agent should take. The observation is an image capturing the current game frame encoded as a NumPy array. As for the actions, they are represented by integers, where 0 describes \"DO NOTHING,\" 1 = \"MOVE LEFT,\" and 2 = \"MOVE RIGHT\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation is represented as Tensor with shape of torch.Size([10, 10, 4]). The first and second dimensions correspond to the height and width, while the third dimension describes the different game objects.\n",
      "Additionally, the data type (dtype) of the observation is torch.float32. Each value in the array indicates whether a particular object is present (1.0 = True) or not present (0.0 = False).\n",
      "The current action is 1 = MOVE LEFT\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get some arbitrary sample\n",
    "obs, act = dataset[10]\n",
    "\n",
    "# Actions and their meaning\n",
    "ACTIONS = {\n",
    "    0: \"DO NOTHING\",\n",
    "    1: \"MOVE LEFT\",\n",
    "    2: \"MOVE RIGHT\",\n",
    "}\n",
    "\n",
    "def to_rgb(obs):\n",
    "    \"\"\" Converts the observation into an rgb image. Taken from MinAtar. \"\"\"\n",
    "    obs = obs.bool().numpy()\n",
    "    n_channels = obs.shape[-1]\n",
    "    cmap = sns.color_palette(\"cubehelix\", n_channels)\n",
    "    cmap.insert(0, (0,0,0))\n",
    "    numerical_state = np.amax(obs * np.reshape(np.arange(n_channels) + 1, (1,1,-1)), 2)\n",
    "    rgb_array = np.stack(cmap)[numerical_state]\n",
    "    return rgb_array\n",
    "\n",
    "print(f\"The observation is represented as {obs.__class__.__name__} with shape of {obs.shape}. The first and second dimensions correspond to the height and width, while the third dimension describes the different game objects.\")\n",
    "print(f\"Additionally, the data type (dtype) of the observation is {obs.dtype}. Each value in the array indicates whether a particular object is present (1.0 = True) or not present (0.0 = False).\")\n",
    "print(f\"The current action is {act} = {ACTIONS[act.item()]}\")\n",
    "\n",
    "# Render observation\n",
    "img = to_rgb(obs)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Define Convolutional Neural Network**\n",
    "\n",
    "Here we define our own classifier that takes as input a batch of observations, and ouputs three logits for each observation. These logits can be fed into a softmax function later to obtain probabilities for the actions. However, it is more convenient for the ``CrossEntropyLoss`` used in classifiers to retrieve the raw logits. We want our model to have two convolutional layers followed by two linear layers. Each but the last layer should be followed by a Rectified Linear Unit (ReLU) activation function.\n",
    "\n",
    "Build a neural network, similar as in 1.3. Notice: it has to take as input 4-channel images. Also, we want the convolutional layers to output the same image height and width and therefore use a stride of one. However, because the kernel size is >1, padding around the images is needed. Visualize to yourself how the kernel size is leading to a change of output size and what padding is needed to keep the dimensions constant. See the PyTorch documentation for convolutional layers to get hints for the padding argument.\n",
    "\n",
    "<img src=\"padding.gif\" alt=\"MinAtar\" width=\"50%\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Defining a CNN**\n",
    "\n",
    "Fill in the gaps in the following code.\n",
    "\n",
    "1. Define the first convolutional layer with 3 input channels (given by the data) and 16 output channels, stride 1, and kernel size 3. (0.5)\n",
    "    \n",
    "    Tipp: Apply padding, such that the image dimensions stay the same\n",
    "\n",
    "2. Define the second convolutional layer with 32 output channels, stride 1, and kernel size 3. (0.5)\n",
    "\n",
    "    Tipp: Apply padding, such that the image dimensions stay the same\n",
    "\n",
    "3. Define the first linear layer mapping from the flattened output of convolutional layer 2, to 128 output nodes. (0.5)\n",
    "\n",
    "    Tipp: How can the input nodes be retrieved by the ouput channels of convolutional layer 2 and the input data shape?\n",
    "\n",
    "4. Define the last linear layer (0.5)\n",
    "\n",
    "    Tipp: How many output nodes do we need given the action space?\n",
    "\n",
    "5. Define the forward pass function by putting all the layers together (1)\n",
    "\n",
    "(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, obs_shape):\n",
    "        # obs_shape is the shape of a single observation --> use this information to define the dimensions of the layers\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(obs_shape[-1], 16, stride=1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, stride=1, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(int(32*obs_shape[0]*obs_shape[1]), 128)\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "\n",
    "        x = torch.flatten(x, 1) # flatten the intermediate result such that it can serve as input for the first linear layer\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "net = CNN(obs_shape=obss.shape[1:])\n",
    "if torch.cuda.is_available():\n",
    "    net.to('cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Define loss function and optimizer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Defining a loss function**\\\n",
    "Implement a cross entropy loss function (1)\n",
    "\n",
    "Hint: check out the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Defining an optimizer**\\\n",
    "Implement an adam optimizer and pass the learning rate as argument (1)\n",
    "\n",
    "Hint: check out the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Train network**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Training the CNN**\\\n",
    "Fill in the gaps in the following code. (2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | loss: 0.006945 | accuracy:  0.57\n",
      "2 | loss: 0.005713 | accuracy:  0.68\n",
      "3 | loss: 0.005272 | accuracy:  0.71\n",
      "4 | loss: 0.005063 | accuracy:  0.72\n",
      "5 | loss: 0.004917 | accuracy:  0.73\n",
      "6 | loss: 0.004828 | accuracy:  0.73\n",
      "7 | loss: 0.004754 | accuracy:  0.74\n",
      "8 | loss: 0.004689 | accuracy:  0.74\n",
      "9 | loss: 0.004638 | accuracy:  0.74\n",
      "10 | loss: 0.004591 | accuracy:  0.75\n",
      "11 | loss: 0.004554 | accuracy:  0.75\n",
      "12 | loss: 0.004523 | accuracy:  0.75\n",
      "13 | loss: 0.004488 | accuracy:  0.75\n",
      "14 | loss: 0.004472 | accuracy:  0.75\n",
      "15 | loss: 0.004437 | accuracy:  0.76\n",
      "16 | loss: 0.004411 | accuracy:  0.76\n",
      "17 | loss: 0.004389 | accuracy:  0.76\n",
      "18 | loss: 0.004374 | accuracy:  0.76\n",
      "19 | loss: 0.004350 | accuracy:  0.76\n",
      "20 | loss: 0.004334 | accuracy:  0.76\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "acc_hist = []\n",
    "\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels] and move to GPU if available\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        # perform the forward pass\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # calculate the loss ussing the criterion\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # perform the backward pass to compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update the model's parameters using the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'{epoch + 1} | loss: {running_loss / total:.6f} | accuracy:  {correct/total:.2f}')\n",
    "    \n",
    "    loss_hist.append(running_loss / total)\n",
    "    acc_hist.append(correct/total)\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Plotting the training results**\\\n",
    "Plot the accuracy and loss over training steps. Make sure to give meaningful axes labels. If you are not familiar with ``matplotlib``, see this [quick start guide](https://matplotlib.org/stable/users/explain/quick_start.html#quick-start). (1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(8,3))\n",
    "\n",
    "# plot accuracy over epochs\n",
    "axs[0].plot(acc_hist)\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].set_ylabel('accuracy')\n",
    "\n",
    "\n",
    "# plot loss over epochs\n",
    "axs[1].plot(loss_hist)\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].set_ylabel('loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Interpreting training results**\n",
    "1. Did the training converge? Why do you think so? (0.5)\n",
    "2. After how many epochs could we have stopped the training? (0.5)\n",
    "\n",
    "(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5 Test network on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test observations: 73 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test observations: {100 * correct // total} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performance is significantly better than random chance, which would be a 33% accuracy when randomly selecting from 3 classes. It appears that the network has learned something meaningful.\n",
    "\n",
    "Lastly, we want to create a short GIF showing our agent playing the breakout task. You don't need to understand the details of how this function works yet. Notice: It saves each game frame as breakout_minatar.png and removes the file in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio.v2 as imageio\n",
    "import gymnasium as gym\n",
    "from matplotlib import colors\n",
    "\n",
    "\n",
    "def create_gif():\n",
    "    \"\"\" Creates a GIF of the agent playing in the current folder. \"\"\"\n",
    "    # Create environment and reset\n",
    "    env = gym.make('MinAtar/Breakout-v1')\n",
    "    obs, info = env.reset()\n",
    "    images = []\n",
    "\n",
    "    for i in range(1_000):\n",
    "        # Get action\n",
    "        obs = torch.tensor(obs, dtype=torch.float).unsqueeze(0)\n",
    "        if torch.cuda.is_available():\n",
    "            obs = obs.cuda()\n",
    "        act_probs = net(obs)\n",
    "        act = np.argmax(act_probs.detach().cpu().numpy())\n",
    "\n",
    "        # Step the environment\n",
    "        obs, reward, terminated, truncated, info = env.step(act)\n",
    "        \n",
    "        # Render as image\n",
    "        n_channels = 4\n",
    "        obs = obs.astype(np.float32)\n",
    "        cmap = sns.color_palette(\"cubehelix\", n_channels)\n",
    "        cmap.insert(0, (0,0,0))\n",
    "        cmap = colors.ListedColormap(cmap)\n",
    "        numerical_state = np.amax(obs * np.reshape(np.arange(n_channels) + 1, (1,1,-1)), 2) + 0.5\n",
    "        numerical_state = numerical_state.repeat(48, axis=0).repeat(48, axis=1)\n",
    "        plt.imsave('breakout_minatar.png', numerical_state, cmap=cmap)\n",
    "        images.append(imageio.imread('breakout_minatar.png'))\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # Save GIF\n",
    "    imageio.mimsave('breakout.gif', images, loop=0, duration=20)\n",
    "    os.remove('breakout_minatar.png')\n",
    "    env.close()\n",
    "\n",
    "create_gif()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6 Additional questions**\n",
    "\n",
    "So far, the hyperparameters for the training were given. We now want to investigate how different hyperparameters influence the training results. Additionally, there are many techniques and architectures to improve robustness and efficiency of neural network training. It is time for you to explore and find a few more.\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Ablation studies: Effect of hyperparameters**\n",
    "1. Learning rate: Test different learning rates of the Adam Optimizer. What happens if the learaning rate is too large/too small? (1)\n",
    "2. Epochs: Can you think of any drawbacks of training a network for too many epochs? If yes, how can we adjust the training to cope for these drawbacks? (1)\n",
    "3. Number of latent channels: Try different values for the number of latent channels. What happens if you increase/decrease the number of channels? Does an increase of channels increase the paramters of a convolutional layer or linear layer more and why? (1)\n",
    "\n",
    "(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "\n",
    "1. Too large: updates are to large and learning does not converge/diverge. Too small: updates too small - takes very long.\n",
    "2. Too many epochs - large power/computation consumption. More importantly, it leads to overfitting to the traning data. Stop training on convergence criterion or better, use a validation dataset to track when the validation performance is at minimum.\n",
    "3. More channles leads to better performance but also longer training time. More channles have a larger effect on linear layers, because there is no weight sharing in linear layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Robustness and efficiency**\n",
    "1. We investigated the effect of layer channels. What other way can you think of to make a neural net more complex/expressive than increasing the channel sizes? (1)\n",
    "2. What is underfitting/overfitting? Explain in a few short sentences. (1)\n",
    "3. Name one technique to prevent underfitting, and one to prevent overfitting. (1)\n",
    "\n",
    "(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "\n",
    "1. Increase the network depth/number of layers. Use more complex layers.\n",
    "2. Underfitting: network did not learn the task. Overfitting: Network performs well on training but poor on test data because weights are over optimized on training data.\n",
    "3. Underfitting: make network more expressive/complex or trian longer. Overfitting: e.g. implement validation dataset or dropout.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
