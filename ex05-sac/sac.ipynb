{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - SAC\n",
    "In this last exercise we will train our own agent using [Soft Actor Critic](https://arxiv.org/pdf/1801.01290.pdf). Currently it is one of the state-of-the-art methods for continuous control with similarities to DDPG and TD3 from the last exercise. Once again we will solve the LunarLander environment from Gymnasium to safely land our vehicle on the surface on the moon.\n",
    "\n",
    "Note: A good introduction to SAC can be found at the [Spinning Up project](https://spinningup.openai.com/en/latest/algorithms/sac.html).\n",
    "\n",
    "<img src=\"resources/lunar_lander.gif\" alt=\"Gymnasium\" width=\"20%\"/>\n",
    "\n",
    "<!--  -->\n",
    "_Agent using random actions to land the lunarlander_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Setup\n",
    "These are the same packages as in the last exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install gymnasium\n",
    "!pip install \"gymnasium[box2d]\"\n",
    "!pip install matplotlib\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import Iterable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n",
    "from collections import namedtuple\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 SAC\n",
    "Soft-Actor Critic (SAC) is an off-policy actor-critic RL method, which maximizes an objective with **entropy regularization**: $J = J_θ + α H(π_θ)$, where $J_θ$ is the expected return with respect to the parameters $\\theta$ of our policy, which we typically maximize, $H$ is the entropy and $\\alpha$ is a hyperparameter to balance it. \n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **SAC Warm-up**\n",
    "1. What does the the Shannon entropy $H(X)$ of a random variable $X$ describe? (0.5 points) How can we calculate it? (0.5 points)\n",
    "2. Let $\\pi(a|s)$ be a stochastic policy that is already partially trained to some task. What happens if we minimize or maximize its entropy, respectively? (1 point)\n",
    "3. What is a normal distribution? Is it bounded? (1 point)\n",
    "4. Describe the difference between $\\epsilon$-greedy and softmax exploration. When should we model our policy with a Gaussian instead of a softmax? (1.5 points)\n",
    "5. What are the consequences of overfitting our policy to specific actions, especially if we want to use it for exploration in our environment? (1 point)\n",
    "6. Looking at TD3 algorithm from the last excercise, explain how using two Q-networks benifit the training process? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "1. ..\n",
    "2. ..\n",
    "3. ..\n",
    "4. ..\n",
    "5. ..\n",
    "6. .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Critic\n",
    "We reuse the Critic implementation from last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int):\n",
    "        \"\"\"\n",
    "        Initialize the Critic network.\n",
    "        \n",
    "        :param obs_dim: dimention of the observations\n",
    "        :param num_actions: dimention of the actions\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim + action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([obs, action], dim=-1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        out = self.fc3(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Actor \n",
    "The actor is also explicitly represented by a neural network. In contrast to DDPG, SAC uses a stochastic policy. This means that the actor outputs a probability distribution over the actions. The distribution is parameterized by a mean $\\mu$ and a standard deviation $\\sigma$. Typically, the stochastic policy is modeled with a squashed diagonal Gaussian distribution. The squashing function is used to ensure that the actions are within the bounds of the environment's action space.\n",
    "\n",
    "### Reparameterization Trick\n",
    "The reason for using a stochastic policy is to allow exploration. However, the stochasticity (sampling directly from $\\mathcal{N}(\\mu(s), \\sigma(s))$) makes it difficult to compute the gradients of the loss function. This is where the reparameterization trick comes in. The trick is to sample the action from a deterministic distribution ($\\mu(s)$ and $\\sigma(s)$) and then add noise ($\\mathcal{N}(0, \\mathcal{I})$). This way, the gradients can be computed with respect to the deterministic distribution, while the actions are still sampled from the stochastic distribution.\n",
    "\n",
    "$$\n",
    "a \\sim \\mathcal{N}(\\mu(s), \\sigma(s)) => \\mu(s) + \\sigma(s) \\cdot \\mathcal{N}(0, \\mathcal{I})\n",
    "$$\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Implementing the Actor**\n",
    "\n",
    "In this exercise we use state based (not image based) observations. \n",
    "The actor should take in an observation and output an action.\n",
    "\n",
    "**NOTE**: The output of `fc_std` should pass through a `softplus` activation so that the standard deviation is non-negative. Finally, the sampled actions should pass through a `tanh` activation and be scaled and translated according to the action space of the environment. The tanh activation restricts the actions to the [-1,1] range, and the scaling and translation ensures that the action is within the boundaries of the environment's action space.\n",
    "\n",
    "- fc1: input:(obs_dim), output: 256\n",
    "- fc_mu: input: 256, output: action_dim\n",
    "- fc_std: input: 256, output: action_dim\n",
    "\n",
    "The reparameterization trick in Pytorch can be implemented using the [rsample](https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.rsample) method of the $\\mathcal{N}(\\mu(s), \\sigma(s))$ distribution object.\n",
    "\n",
    "The log probability of the action can be calculated using the [log_prob](https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.log_prob) method of the $\\mathcal{N}(\\mu(s), \\sigma(s))$ distribution object.\n",
    "\n",
    "action_high and action_low can be determined by examining the env using ``env.action_space.high`` and ``env.action_space.low``.\n",
    "\n",
    "Implement this architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, action_low: np.array, action_high: np.array):\n",
    "        \"\"\"\n",
    "        Initialize the Actor network.\n",
    "        \n",
    "        :param obs_dim: dimention of the observations\n",
    "        :param num_actions: dimention of the actions\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        # We are registering scale and bias as buffers so they can be saved and loaded as part of the model.\n",
    "        # Buffers won't be passed to the optimizer for training!\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((action_high - action_low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((action_high + action_low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        \n",
    "        # TODO: your code\n",
    "        self.fc1 = ...\n",
    "        self.fc_mu = ...\n",
    "        self.fc_std = ...\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the actor network.\n",
    "        \n",
    "        return: mean_action, log_prob_action\n",
    "        \"\"\"\n",
    "        # TODO: your code\n",
    "        ...\n",
    "        mu = ...\n",
    "        std = ...\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        action = ...\n",
    "        log_prob = ...\n",
    "\n",
    "        # Enforcing action bounds\n",
    "        adjusted_action = torch.tanh(action) * self.action_scale + self.action_bias\n",
    "        adjusted_log_prob = log_prob - torch.log(self.action_scale * (1-torch.tanh(action).pow(2)) + 1e-6)\n",
    "        return adjusted_action, adjusted_log_prob\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "We reuse the implementation of our replay buffer from last exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size: int):\n",
    "        \"\"\"\n",
    "        Create the replay buffer.\n",
    "\n",
    "        :param max_size: Maximum number of transitions in the buffer.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.max_size = max_size\n",
    "        self.position = 0\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns how many transitions are currently in the buffer.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def store(self, obs: torch.Tensor, action: torch.Tensor, reward: torch.Tensor, next_obs: torch.Tensor, terminated: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Adds a new transition to the buffer. When the buffer is full, overwrite the oldest transition.\n",
    "\n",
    "        :param obs: The current observation.\n",
    "        :param action: The action.\n",
    "        :param reward: The reward.\n",
    "        :param next_obs: The next observation.\n",
    "        :param terminated: Whether the episode terminated.\n",
    "        \"\"\"\n",
    "        if len(self.data) < self.max_size:\n",
    "            self.data.append((obs, action, reward, next_obs, terminated))\n",
    "        else:\n",
    "            self.data[self.position] = (obs, action, reward, next_obs, terminated)   \n",
    "        self.position = (self.position + 1) % self.max_size         \n",
    "\n",
    "    def sample(self, batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions uniformly and with replacement. The respective elements e.g. states, actions, rewards etc. are stacked\n",
    "\n",
    "        :param batch_size: The batch size.\n",
    "        :returns: A tuple of tensors (obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch), where each tensors is stacked.\n",
    "        \"\"\"\n",
    "        return [torch.stack(b) for b in zip(*random.choices(self.data, k=batch_size))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Algorithm\n",
    "In this section, we will first look at the update formulas of SAC and then implement the entire algorithm.\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Updating the critics**\n",
    "\n",
    "Implement the function ``update_critics`` which gets the sampled data from the replay buffer, calculates the loss of both critics, respectively, and performs an update step.\n",
    "\n",
    "Note: As our objective has changed, our Q-function also changes as well to include the entropy bonus. The target for our critics is given by (from Spinning Up):\n",
    "\n",
    "$y = r + \\gamma(1-tm)(\\min_{j=1,2}Q_{\\text{targ, j}}(s', \\tilde{a}') - \\alpha \\log \\pi_\\theta(\\tilde{a}'|s'))$\n",
    "with action $\\tilde{a}' \\sim \\pi_\\theta(\\cdot | s')$ sampled from our actor\n",
    "\n",
    "Be aware, we are using two target networks and take the minimum over both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_critics(\n",
    "        q1: nn.Module,\n",
    "        q1_target: nn.Module,\n",
    "        q1_optimizer: optim.Optimizer,\n",
    "        q2: nn.Module,\n",
    "        q2_target: nn.Module,\n",
    "        q2_optimizer: optim.Optimizer,\n",
    "        actor_target: nn.Module,\n",
    "        log_ent_coef: torch.Tensor,\n",
    "        gamma: float,\n",
    "        obs: torch.Tensor,\n",
    "        act: torch.Tensor,\n",
    "        rew: torch.Tensor,\n",
    "        next_obs: torch.Tensor,\n",
    "        tm: torch.Tensor,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Update both of SAC's critics for one optimizer step.\n",
    "\n",
    "    :param q1: The first critic network.\n",
    "    :param q1_target: The target first critic network.\n",
    "    :param q1_optimizer: The first critic's optimizer.\n",
    "    :param q2: The second critic network.\n",
    "    :param q2_target: The target second critic network.\n",
    "    :param q2_optimizer: The second critic's optimizer.\n",
    "    :param actor: The actor network.\n",
    "    :param actor_target: The target actor network.\n",
    "    :param actor_optimizer: The actor's optimizer.\n",
    "    :param gamma: The discount factor.\n",
    "    :param obs: Batch of current observations.\n",
    "    :param act: Batch of actions.\n",
    "    :param rew: Batch of rewards.\n",
    "    :param next_obs: Batch of next observations.\n",
    "    :param tm: Batch of termination flags.\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO: 1. Calculate the target\n",
    "    with torch.no_grad():\n",
    "       ...\n",
    "    \n",
    "    # TODO: 2. Update both q function using our target \n",
    "    for q, optimizer in [(q1, q1_optimizer), (q2, q2_optimizer)]:\n",
    "        ...\n",
    "\n",
    "\n",
    "def update_actor(\n",
    "    q1: nn.Module,\n",
    "    q2: nn.Module,\n",
    "    actor: nn.Module,\n",
    "    actor_optimizer: optim.Optimizer,\n",
    "    obs: torch.Tensor,\n",
    "    log_ent_coef: torch.Tensor, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Update the SAC's Actor network for one optimizer step.\n",
    "\n",
    "    :param critic: The critic network.\n",
    "    :param actor: The actor network.\n",
    "    :param actor_optimizer: The actor's optimizer.\n",
    "    :param obs: Batch of current observations.\n",
    "\n",
    "    \"\"\"\n",
    "    # Actor Update\n",
    "    action, action_log_prob = actor(obs)\n",
    "    entropy = - log_ent_coef.exp() * action_log_prob\n",
    "    q1, q2 = q1(obs, action), q2(obs, action)\n",
    "    q1_q2 = torch.cat([q1, q2], dim=1)\n",
    "    min_q = torch.min(q1_q2, 1, keepdim=True)[0]\n",
    "    actor_loss = (- min_q - entropy).mean()\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "\n",
    "def update_entropy_coefficient(\n",
    "        actor: nn.Module,\n",
    "        log_ent_coef: torch.Tensor,\n",
    "        target_entropy: float,\n",
    "        ent_coef_optimizer: optim.Optimizer,\n",
    "        obs: torch.Tensor,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Automatic update for entropy coefficient (alpha)\n",
    "\n",
    "    :param actor: the actor network.\n",
    "    :param log_ent_coef: tensor representing the log of entropy coefficient (log_alpha).\n",
    "    :param target_entropy: tensor representing the desired target entropy.\n",
    "    :param ent_coef_optimizer: torch optimizer for entropy coefficient.\n",
    "    :param obs: current batch observation.\n",
    "    \"\"\"\n",
    "    _, action_log_prob = actor(obs)\n",
    "    ent_coef_loss = -(log_ent_coef.exp() * (action_log_prob + target_entropy).detach()).mean()\n",
    "    ent_coef_optimizer.zero_grad()\n",
    "    ent_coef_loss.backward()\n",
    "    ent_coef_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Polyak Update of the target networks\n",
    "It is common to update the target networks very slowly at every step. We use the polyak update from last exercise again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyak_update(\n",
    "    params: Iterable[torch.Tensor],\n",
    "    target_params: Iterable[torch.Tensor],\n",
    "    tau: float,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform a Polyak average update on ``target_params`` using ``params``:\n",
    "\n",
    "    :param params: parameters of the original network (model.parameters())\n",
    "    :param target_params: parameters of the target network (model_target.parameters())\n",
    "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1) 1 -> Hard update, 0 -> No update\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param, target_param in zip(params, target_params):\n",
    "            target_param.data.mul_(1 - tau)\n",
    "            torch.add(target_param.data, param.data, alpha=tau, out=target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now putting it all together. This implementation is very similar to the DDPG one. \n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Implementing the SAC Agent**\n",
    "\n",
    "Implement the SAC agent by filling in the missing gaps in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EpisodeStats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n",
    "\n",
    "\n",
    "class SACAgent:\n",
    "    def __init__(self,\n",
    "            env,\n",
    "            gamma=0.99,\n",
    "            lr=0.001, \n",
    "            batch_size=64,\n",
    "            tau=0.005,\n",
    "            maxlen=100_000,\n",
    "            target_entropy=-1.0,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the SAC agent.\n",
    "\n",
    "        :param env: The environment.\n",
    "        :param exploration_noise.\n",
    "        :param gamma: The discount factor.\n",
    "        :param lr: The learning rate.\n",
    "        :param batch_size: Mini batch size.\n",
    "        :param tau: Polyak update coefficient.\n",
    "        :param max_size: Maximum number of transitions in the buffer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self.target_entropy=target_entropy\n",
    "\n",
    "        # Initialize the Replay Buffer\n",
    "        self.buffer = ReplayBuffer(maxlen)\n",
    "\n",
    "        # TODO: Initialize two critic and one actor network\n",
    "        self.q1 = ...\n",
    "        self.q2 = ...\n",
    "        self.actor = ...\n",
    "        self.log_ent_coef = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "        # TODO: Initialze two target critic and one target actor networks and load the corresponding state_dicts\n",
    "        self.q1_target = ...\n",
    "        ...\n",
    "        self.q2_target = ...\n",
    "        ...\n",
    "        self.actor_target = ...\n",
    "        ...\n",
    "        \n",
    "        # TODO: Create ADAM optimizer for the Critic and Actor networks\n",
    "        self.q1_optimizer = ...\n",
    "        self.q2_optimizer = ...\n",
    "        self.actor_optimizer = ...\n",
    "        self.ent_coef_optimizer = optim.Adam([self.log_ent_coef], lr=lr)\n",
    "\n",
    "\n",
    "    def train(self, num_episodes: int) -> EpisodeStats:\n",
    "        \"\"\"\n",
    "        Train the SAC agent.\n",
    "\n",
    "        :param num_episodes: Number of episodes to train.\n",
    "        :returns: The episode statistics.\n",
    "        \"\"\"\n",
    "        # Keeps track of useful statistics\n",
    "        stats = EpisodeStats(\n",
    "            episode_lengths=np.zeros(num_episodes),\n",
    "            episode_rewards=np.zeros(num_episodes),\n",
    "        )\n",
    "        current_timestep = 0\n",
    "\n",
    "        for i_episode in range(num_episodes):\n",
    "            # Print out which episode we're on, useful for debugging.\n",
    "            if (i_episode + 1) % 100 == 0:\n",
    "                print(f'Episode {i_episode + 1} of {num_episodes}  Time Step: {current_timestep}')\n",
    "\n",
    "            # Reset the environment and get initial observation\n",
    "            obs, _ = self.env.reset()\n",
    "            \n",
    "            for episode_time in itertools.count():\n",
    "                # Choose action and execute\n",
    "                with torch.no_grad():\n",
    "                    action, _ = self.actor(torch.as_tensor(obs).float())\n",
    "                    action = action.cpu().numpy().clip(self.env.action_space.low, self.env.action_space.high)\n",
    "                next_obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                # Update statistics\n",
    "                stats.episode_rewards[i_episode] += reward\n",
    "                stats.episode_lengths[i_episode] += 1\n",
    "\n",
    "                # Store sample in the replay buffer\n",
    "                self.buffer.store(\n",
    "                    torch.as_tensor(obs, dtype=torch.float32),\n",
    "                    torch.as_tensor(action),\n",
    "                    torch.as_tensor(reward, dtype=torch.float32),\n",
    "                    torch.as_tensor(next_obs, dtype=torch.float32),\n",
    "                    torch.as_tensor(terminated),\n",
    "                )\n",
    "\n",
    "                # Sample a mini batch from the replay buffer\n",
    "                obs_batch, act_batch, rew_batch, next_obs_batch, tm_batch = self.buffer.sample(self.batch_size)\n",
    "\n",
    "                # Update the Critic network\n",
    "                ...\n",
    "\n",
    "                # Update the Actor network\n",
    "                update_actor(\n",
    "                    self.q1,\n",
    "                    self.q2,\n",
    "                    self.actor,\n",
    "                    self.actor_optimizer,\n",
    "                    obs_batch.float(),\n",
    "                    self.log_ent_coef,\n",
    "                )\n",
    "                \n",
    "                # Update Entropy Coefficient\n",
    "                update_entropy_coefficient(\n",
    "                    self.actor,\n",
    "                    self.log_ent_coef,\n",
    "                    self.target_entropy,\n",
    "                    self.ent_coef_optimizer,\n",
    "                    obs_batch.float(),\n",
    "                )\n",
    "\n",
    "                # Update the target networks via Polyak Update\n",
    "                polyak_update(self.q1.parameters(), self.q1_target.parameters(), self.tau)\n",
    "                polyak_update(self.q2.parameters(), self.q2_target.parameters(), self.tau)\n",
    "                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n",
    "                \n",
    "                current_timestep += 1\n",
    "\n",
    "                # Check whether the episode is finished\n",
    "                if terminated or truncated or episode_time >= 500:\n",
    "                    break\n",
    "                obs = next_obs\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Training\n",
    "We train now on the Lunar Lander environment from (Gymnasium)[https://gymnasium.farama.org/environments/box2d/lunar_lander/]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your environment\n",
    "env = gym.make(\"LunarLander-v2\",\n",
    "    continuous = True,\n",
    "    gravity = -10.0,\n",
    "    render_mode=\"rgb_array\")\n",
    "# Print observation and action space infos\n",
    "print(f\"Training on {env.spec.id}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\\n\")\n",
    "\n",
    "# Hyperparameters, Hint: Change as you see fit\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_BUFFER_SIZE = 100_000\n",
    "TAU = 0.005\n",
    "NUM_EPISODES = 1_000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "TARGET_ENTROPY = -1.0\n",
    "\n",
    "# Train SAC\n",
    "agent = SACAgent(\n",
    "    env, \n",
    "    gamma=DISCOUNT_FACTOR,\n",
    "    lr=LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    tau=TAU,\n",
    "    maxlen=REPLAY_BUFFER_SIZE,\n",
    "    target_entropy=TARGET_ENTROPY,\n",
    ")\n",
    "stats = agent.train(NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load the trained actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained actor\n",
    "torch.save(agent.actor, \"sac_actor.pt\")\n",
    "\n",
    "# loading the trained actor\n",
    "loaded_actor = torch.load(\"sac_actor.pt\")\n",
    "loaded_actor.eval()\n",
    "print(loaded_actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Results\n",
    "\n",
    "Like in the last exercise, we will look at the resulting episode reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_window=20\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), tight_layout=True)\n",
    "\n",
    "# Plot the episode length over time\n",
    "ax = axes[0]\n",
    "ax.plot(stats.episode_lengths)\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episode Length\")\n",
    "ax.set_title(\"Episode Length over Time\") \n",
    "\n",
    "# Plot the episode reward over time\n",
    "ax = axes[1]\n",
    "rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "ax.plot(rewards_smoothed)\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episode Reward (Smoothed)\")\n",
    "ax.set_title(f\"Episode Reward over Time\\n(Smoothed over window size {smoothing_window})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let us see what the learned policy does in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IImage\n",
    "\n",
    "def save_rgb_animation(rgb_arrays, filename, duration=50):\n",
    "    \"\"\"Save an animated GIF from a list of RGB arrays.\"\"\"\n",
    "    # Create a list to hold each frame\n",
    "    frames = []\n",
    "\n",
    "    # Convert RGB arrays to PIL Image objects\n",
    "    for rgb_array in rgb_arrays:\n",
    "        rgb_array = (rgb_array).astype(np.uint8)\n",
    "        img = Image.fromarray(rgb_array)\n",
    "        frames.append(img)\n",
    "    \n",
    "    # Save the frames as an animated GIF\n",
    "    frames[0].save(filename, save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
    "\n",
    "def rendered_rollout(policy, env, max_steps=1_000):\n",
    "    \"\"\"Rollout for one episode while saving all rendered images.\"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    imgs = [env.render()]\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        with torch.no_grad():\n",
    "            action = policy(torch.as_tensor(obs, dtype=torch.float32))[0].cpu().numpy()\n",
    "            \n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        imgs.append(env.render())\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return imgs\n",
    "\n",
    "imgs = rendered_rollout(loaded_actor, env)\n",
    "save_rgb_animation(imgs, \"trained.gif\")\n",
    "IImage(filename=\"trained.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not train very far because it can be very time-consuming.\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **RL Project Preperation**\n",
    "\n",
    "1. What aspects of the problem should be first considered when trying to use RL for a particular task? (1 point)\n",
    "2. Which quantities can be monitored during training in order to observe progress and help with debugging? (1 point)\n",
    "3. What are some real world applications of RL? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "1. ..\n",
    "2. ..\n",
    "3. .."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
