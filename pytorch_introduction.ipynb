{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction into PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a Python-based library designed for deep learning. It is distinguished by its dynamic computational graph, which enables researchers and developers to construct models with a high degree of flexibility. PyTorch has found extensive use in various scientific and engineering domains due to its ease of use and extensive research-friendly features.\n",
    "\n",
    "This notebook is based on the [PyTorch 60-Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Tensors\n",
    "Tensors are a specialized data structure similar to arrays and matrices. They are comparable to NumPy's ndarrays, but they have the advantage that PyTorch tensors can run on GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Tensor Initialization**\n",
    "\n",
    "There are many different ways of initialising tensors e.g. from data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy: \n",
      " [[1 2]\n",
      " [3 4]] \n",
      "\n",
      "PyTorch: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating ndarray/tensor from data\n",
    "n = np.array([[1, 2], [3, 4]])\n",
    "t = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "print(f\"NumPy: \\n {n} \\n\")\n",
    "print(f\"PyTorch: \\n {t} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or creating them from constant or random values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " array([[1., 1., 1.],\n",
       "        [1., 1., 1.]]),\n",
       " array([[0.5720494 , 0.90804453, 0.00744775],\n",
       "        [0.72319214, 0.32139422, 0.85191981]]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[0.5425, 0.8958, 0.4804],\n",
       "         [0.9227, 0.7835, 0.3031]]),\n",
       " tensor([[3.1415, 3.1415, 3.1415],\n",
       "         [3.1415, 3.1415, 3.1415]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NumPy\n",
    "n_zeros  = np.zeros((2, 3))\n",
    "n_ones   = np.ones((2, 3))\n",
    "n_random = np.random.random((2, 3))\n",
    "\n",
    "# PyTorch, No parantheses around the shape needed!\n",
    "t_zeros  = torch.zeros(2, 3)\n",
    "t_ones   = torch.ones(2, 3)\n",
    "t_random = torch.rand(2, 3)\n",
    "t_full = torch.full((2, 3), 3.1415)\n",
    "\n",
    "n_zeros, n_ones, n_random, t_zeros, t_ones, t_random, t_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Conversion between NumPy and PyTorch**\n",
    "\n",
    "The conversion of an ndarray into a torch tensor is simple. Both will share the same memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy: [1. 1. 1.]\n",
      "PyTorch: tensor([1., 1., 1.], dtype=torch.float64)\n",
      "NumPy: [2. 2. 2.]\n",
      "PyTorch: tensor([2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# NumPy -> PyTorch\n",
    "n = np.ones((3))\n",
    "t = torch.from_numpy(n)\n",
    "\n",
    "print(f\"NumPy: {n}\")\n",
    "print(f\"PyTorch: {t}\")\n",
    "\n",
    "# Same memory, so operations will affect both!\n",
    "t += 1\n",
    "\n",
    "print(f\"NumPy: {n}\")\n",
    "print(f\"PyTorch: {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy: [1. 1. 1.]\n",
      "PyTorch: tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch -> NumPy\n",
    "t = torch.ones(3)\n",
    "n = t.numpy()\n",
    "\n",
    "print(f\"NumPy: {n}\")\n",
    "print(f\"PyTorch: {t}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Tensor Attributes**\n",
    "\n",
    "We can print information such as the tensor shape, the tensor datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([3, 4])\n",
      "Datatype: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "n = np.ones((3, 4))\n",
    "t = torch.ones(3, 4)\n",
    "\n",
    "print(f\"Shape: {t.shape}\")\n",
    "print(f\"Datatype: {t.dtype}\")\n",
    "print(f\"Device tensor is stored on: {t.device}\") # PyTorch tensors can be stored on the GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the shape of the tensors via ``reshape``. Be aware that reshape may return a copy of the original tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(t.shape)\n",
    "\n",
    "# Change shape\n",
    "t = t.reshape(4, 3)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tensor has a specific data type. A list of dtypes can be found here: https://pytorch.org/docs/stable/tensors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.bool\n",
      "torch.int32\n"
     ]
    }
   ],
   "source": [
    "print(t.dtype)\n",
    "\n",
    "# Conversion of dtypes\n",
    "t = t.type(torch.bool)\n",
    "print(t.dtype)\n",
    "\n",
    "# Shorter way\n",
    "t = t.int()\n",
    "print(t.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are normally created on the CPU. We have to move them to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  t = t.to(\"cuda\")\n",
    "  print(f\"Device tensor is stored on: {t.device}\")\n",
    "\n",
    "  t += 1\n",
    "  print(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 Tensor Operations**\n",
    "\n",
    "Many operations that you find in NumPy are also available in PyTorch. Some, however, under a different name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1, 2, 3])\n",
      "First column: tensor([1, 4, 7])\n",
      "Last column: tensor([3, 6, 9])\n",
      "tensor([[1, 0, 3],\n",
      "        [4, 0, 6],\n",
      "        [7, 0, 9]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Indexing and Slicing\n",
    "t = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "print(f\"First row: {t[0]}\")\n",
    "print(f\"First column: {t[:, 0]}\")\n",
    "print(f\"Last column: {t[:, -1]}\")\n",
    "t[:, 1] = 0\n",
    "print(t)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy: [1 2 3 4]\n",
      "PyTorch: tensor([1, 2, 3, 4])\n",
      "\n",
      "NumPy: \n",
      "[[1 2]\n",
      " [3 4]]\n",
      "PyTorch: \n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n0 = np.array([1, 2])\n",
    "n1 = np.array([3, 4])\n",
    "\n",
    "t0 = torch.tensor([1, 2])\n",
    "t1 = torch.tensor([3, 4])\n",
    "\n",
    "# Concatenating\n",
    "print(f\"NumPy: {np.concatenate([n0, n1])}\")\n",
    "print(f\"PyTorch: {torch.cat([t0, t1])}\\n\")\n",
    "\n",
    "# Stacking (along a new dimension)\n",
    "print(f\"NumPy: \\n{np.stack([n0, n1])}\")\n",
    "print(f\"PyTorch: \\n{torch.stack([t0, t1])}\\n\")\n",
    "\n",
    "# Check for shape!\n",
    "t2 = torch.tensor([1, 2, 3])\n",
    "#print(torch.stack([t0, t2]).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[8., 8.],\n",
      "        [8., 8.],\n",
      "        [8., 8.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[8., 8.],\n",
      "        [8., 8.],\n",
      "        [8., 8.]])\n",
      "tensor.matmul(tensor.T) \n",
      " tensor([[16., 16., 16.],\n",
      "        [16., 16., 16.],\n",
      "        [16., 16., 16.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[16., 16., 16.],\n",
      "        [16., 16., 16.],\n",
      "        [16., 16., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# multiplying tensors\n",
    "t0 = torch.full((3, 2), 2.0)\n",
    "t1 = torch.full((3, 2), 4.0)\n",
    "\n",
    "# element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {t0.mul(t1)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {t0 * t1}\")\n",
    "\n",
    "# matrix multiplication\n",
    "print(f\"tensor.matmul(tensor.T) \\n {t0.matmul(t1.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {t0 @ t1.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor before inplace addition \n",
      " tensor([[1, 0, 3],\n",
      "        [4, 0, 6],\n",
      "        [7, 0, 9]]) \n",
      "\n",
      "Tensor after inplace addition \n",
      " tensor([[ 6,  5,  8],\n",
      "        [ 9,  5, 11],\n",
      "        [12,  5, 14]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inplace operations have a trailing underscore\n",
    "print(f\"Tensor before inplace addition \\n {t} \\n\")\n",
    "t.add_(5)\n",
    "print(f\"Tensor after inplace addition \\n {t} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full list of available tensor operations check out the corresponding [PyTorch documentation](https://pytorch.org/docs/stable/torch.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n",
      "tensor(3.)\n",
      "tensor([[-0.6931, -0.6931, -0.6931],\n",
      "        [-0.6931, -0.6931, -0.6931]])\n",
      "tensor([[0.4794, 0.4794, 0.4794],\n",
      "        [0.4794, 0.4794, 0.4794]])\n",
      "tensor([[-9.2500, -9.2500, -9.2500],\n",
      "        [-9.2500, -9.2500, -9.2500]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.full((2, 3), 0.5)\n",
    "\n",
    "# Further operation examples\n",
    "print(t.mean())\n",
    "print(t.sum())\n",
    "print(t.log())\n",
    "print(t.sin())\n",
    "print(t + 2*t ** 3 - 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Introduction to torch.autograd\n",
    "\n",
    "``torch.autograd`` is PyTorch's engine for automatic differentiation. It is essential for the training of neural networks.\n",
    "\n",
    "**2.1 Differentiation in Autograd**\n",
    "\n",
    "The argument ``required_grad=True`` signals to ``autograd`` that every operation on those tensors should be tracked. This allows ``autograd`` to collect gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If ``a`` and ``b`` are parameters of a neural networks the error function ``Q`` could looks like this:\n",
    "\n",
    "\\begin{align}Q = 3a^3 - b^2\\end{align}\n",
    "\n",
    "For the training of the neural network we need to calculate the gradients with respect to the parameters: \n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial a} = 9a^2\\end{align}\n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial b} = -2b\\end{align}\n",
    "\n",
    "With ``autograd`` you can calculate those gradients by calling ``.backward()`` on Q. The ``gradient`` argument is used here to specify how much the tensors ``a`` and ``b`` should influence the gradient calculation of ``Q``. By providing ``external_grad`` as the gradient argument, you ensure that both ``a`` and ``b`` are treated as if they contribute equally to the gradient of ``Q`` (both having a weight of 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "\n",
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Computational Graph**\n",
    "\n",
    "``autograd`` functions by maintaining a record of both data (tensors) and executed operations in a directed acyclic graph (DAG) composed of Function objects. Within this graph structure, the input tensors serve as the starting point (leaves), and the output tensors act as the endpoints (roots). By traversing this graph in a reverse manner, one can automatically compute gradients using the chain rule.\n",
    "\n",
    "- **Forward pass**: ``autograd`` performs operations to compute the resulting tensor and maintains the operation's gradient function in the DAG.\n",
    "\n",
    "- **Backward pass**: ``autograd`` (triggered by calling ``.backward()`` on the DAG root) computes the gradients from each ``.grad_fn``, accumulates them in the corresponding tensor's ``.grad`` attribute, and applies the chain rule to propagate gradients to the leaf tensors.\n",
    "\n",
    "``autograd`` tracks operations for tensors with ``requires_grad=True``, while setting ``requires_grad=False`` excludes them; if any input tensor has ``requires_grad=True``, the output tensor will also require gradients.\n",
    "\n",
    "Note: In PyTorch, DAGs (Directed Acyclic Graphs) are dynamic, and it's important to know that a new graph is built from scratch after each ``.backward()`` call. This flexibility enables the use of control flow statements and the ability to modify the model's shape, size, and operations in each iteration as required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients?: False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients?: {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Neural Networks with PyTorch\n",
    "\n",
    "The ``torch.nn`` package can be used to construct neural networks. ``nn.Module``s contain layers, and a method ``forward(input)`` that returns the ``output``.\n",
    "\n",
    "Typical training procedure for a neural network:\n",
    "1. Define neural network with some learnable parameters (weights)\n",
    "2. Iterate over dataset of inputs\n",
    "3. Process input through network\n",
    "4. Compute the loss (how wrong is the output)\n",
    "5. Backpropagate to calculate the gradient for each of the network's weights\n",
    "6. Update the weights of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.nn.Module** holds both the **weights** of the network and their corresponding **gradient** tensors. The weights and the gradients can be examined via ```model.parameters()```. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Defining a neural network**\n",
    "\n",
    "In general, a neural network in PyTorch can be described by the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork()\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # SETUP\n",
    "\n",
    "    def forward(self, x):\n",
    "        # FORWARD PASS\n",
    "        return x\n",
    "\n",
    "net = NeuralNetwork()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we define the ``forward`` function, the ``backward`` function is automatically defined, which can be used for training of the network.\n",
    "\n",
    "There are a variety of layers, activation functions etc. to choose from. For an overview, see: https://pytorch.org/docs/stable/nn.html\n",
    "The most important ones for our tasks are the ``Linear`` layer for fully connected layers and the ``Conv2d`` layer for convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([32, 5]), gradient information available? True\n",
      "Output shape: torch.Size([32, 5, 32, 32]), gradient information available? True\n"
     ]
    }
   ],
   "source": [
    "# Linear layer\n",
    "in_features = 3\n",
    "out_features = 5\n",
    "linear_layer = nn.Linear(in_features, out_features)\n",
    "\n",
    "# First dimension is the BATCH dimension, the second the one the number of in_features\n",
    "test_input = torch.rand(32, 3)\n",
    "test_output = linear_layer(test_input)\n",
    "print(f\"Output shape: {test_output.shape}, gradient information available? {test_output.requires_grad}\")\n",
    "\n",
    "\n",
    "# Convolutional layer, no information about the height and width of the input required!\n",
    "in_channels = 3\n",
    "out_channels = 5\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "cnn_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=1, padding_mode='zeros')\n",
    "\n",
    "# First dimension is the BATCH dimension, the second the number of in_channels\n",
    "# The third and forth dimensions are the height and width, respectively\n",
    "test_input = torch.rand(32, 3, 32, 32)\n",
    "test_output = cnn_layer(test_input)\n",
    "print(f\"Output shape: {test_output.shape}, gradient information available? {test_output.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a complete example of a neural network using a combination of convolutions and fully connected layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "Number of learnable parameters: 61706 \n",
      "\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, (5,5))\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension, 32 -> (32-4)/2 -> (14-4)/2 -> 5\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "\n",
    "        # BCHW -> B,N\n",
    "\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "# learnable parameters of the model\n",
    "params = list(net.parameters())\n",
    "n_params = np.sum([torch.numel(p) for p in params])\n",
    "print(f\"Number of learnable parameters: {n_params} \\n\")\n",
    "print(params[0].shape)  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a much shorter version that uses nn.Sequential. It forwards the data through all layers in the given order. However, it is not as flexible as the definition of the network as a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, (5, 5)),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d((2, 2)),\n",
    "    nn.Conv2d(6, 16, (5, 5)),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d((2, 2)),\n",
    "    nn.Flatten(start_dim=1, end_dim=-1),\n",
    "    nn.Linear(16 * 5 * 5, 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(84, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0672, -0.0951,  0.0648, -0.0270,  0.0832, -0.0916, -0.0135,  0.0494,\n",
      "         -0.0704, -0.0135]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Random input\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero gradient buffers of all parameters and backprops with random gradients\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Loss Function**\n",
    "\n",
    "The loss function computes a value that estimates how far away the output is from the target. For the full list of available loss functions check out the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html#loss-functions).\n",
    "\n",
    "Example: ``MSELoss`` (mean squared error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2268, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Backprop**\n",
    "\n",
    "To initiate error backpropagation, use ``loss.backward()``, but make sure to clear existing gradients; otherwise, the new gradients will accumulate onto the existing ones. This step is crucial for accurate gradient calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0143, -0.0089,  0.0054, -0.0013, -0.0050, -0.0194])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print(\"conv1.bias.grad before backward\")\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"conv1.bias.grad after backward\")\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4 Update the weights**\n",
    "\n",
    "A simple update rule is the Stochastic Gradient Descent (SGD):\n",
    "\n",
    "*weight = weight - learning_rate * gradient*\n",
    "\n",
    "The ``torch.optim`` package implements different update rules such as SGB, Nesterov-SGD, Adam, RMSProp, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, you will find the following sequence of operations to train a PyTorch network for one iteration. It consists of zeroing the gradients, performing a forward pass + loss calculation. Calculate the gradients with ``backward()`` and update the parameters of the network using an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your training loop: (IDIOMATIC)\n",
    "optimizer.zero_grad()               # zero the gradient buffers\n",
    "output = net(input)                 # make prediction\n",
    "loss = criterion(output, target)    # calculate loss between prediction and ground truth\n",
    "loss.backward()                     # backpropogate the loss (gradient calculation)\n",
    "optimizer.step()                    # update the weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
